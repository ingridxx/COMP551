{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, data, train_X = None, train_y = None, test_X = None, test_y = None, percentage=0.5, lr=0.01, max_iter=10000, beta=0.99, reg_term=0):  # percentage = train/cv+test split\n",
    "        self.data = data\n",
    "        self.percentage = percentage\n",
    "        if train_X is None and test_X is None:\n",
    "            self.train_X, self.train_y, self.sub_X, self.sub_y = self.split_data(self.data, self.percentage)\n",
    "        else:\n",
    "            self.train_X, self.train_y, self.sub_X, self.sub_y = train_X, train_y, test_X, test_y\n",
    "            \n",
    "        self.test_X, self.test_y, self.cv_X, self.cv_y = self.split_data(pd.concat([self.sub_X,self.sub_y], axis=1), 0.5)\n",
    "        self.thetas = self.gradient_descent(self.train_X.values, self.train_y.values, self.cv_X.values, self.cv_y.values, learning_rate=lr, max_iter=max_iter, beta=beta, reg_term=reg_term)\n",
    "        \n",
    "        self.testing_accuracy = self.get_test_acc(self.test_X.values, self.test_y.values, self.thetas)\n",
    "        \n",
    "    def split_data(self, data, percentage=0.5):\n",
    "        val = np.random.rand(len(data)) < percentage  #splits data and sorts into x, y values\n",
    "        train = data[val]\n",
    "        test = data[~val]\n",
    "\n",
    "        train_X = train.iloc[:, :-1]\n",
    "        train_y = train.iloc[:, -1]\n",
    "\n",
    "        test_X = test.iloc[:, :-1]\n",
    "        test_y = test.iloc[:, -1]\n",
    "        return train_X, train_y ,test_X, test_y\n",
    "    \n",
    "    def predict_proba(self, X, theta):\n",
    "        return self.sigmoid(np.dot(X, theta))\n",
    "\n",
    "    def predict(self, X, theta):\n",
    "        prediction = self.predict_proba(X, theta)\n",
    "        predict_arr = []\n",
    "        for i in prediction:\n",
    "            if i>=0.5:\n",
    "                predict_arr.append(1)\n",
    "            else:\n",
    "                predict_arr.append(0)\n",
    "\n",
    "        return predict_arr\n",
    "\n",
    "    def accuracy(self, predict_arr, y):\n",
    "        correct = 0\n",
    "        for i,j in zip(predict_arr, y):\n",
    "            if i==j[0]:\n",
    "                correct+=1\n",
    "        return correct/len(y)  # accuracy = # tp+tn / total\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def gradient(self, X, y, theta, lambdaa):  # lambdaa is regularization term\n",
    "        N, D = len(X[0]), len(X[0])\n",
    "        yh = self.sigmoid(np.dot(X, theta))\n",
    "        grad = np.dot(X.T, yh-y) / N\n",
    "        grad[1:] += lambdaa * theta[1:]\n",
    "        return grad\n",
    "\n",
    "    def gradient_descent(self, X, y, cv_X, cv_y, learning_rate=0.1, max_iter=10000, beta=0.99, reg_term=0.5):  # attempted termination condition - lack of improvement in cross validation set\n",
    "        N, D = len(X[0]), len(X[0])\n",
    "        theta = np.zeros((len(X[0]), 1))\n",
    "        y = np.reshape(y, (-1,1))  # creates two-dimensional array\n",
    "        cv_y = np.reshape(cv_y, (-1,1))\n",
    "        iterate, cv_acc, prev_cv_acc, d_theta = 0, 0, 0, 0\n",
    "        max_cv_acc = 0  # maximum cross validation accuracy - records thetas at highest cv_acc \n",
    "        best_theta = theta\n",
    "        g = np.inf\n",
    "        eps = 1e-2\n",
    "        while (np.linalg.norm(g) > eps):  # can add in 'or cv_acc>=prev_cv_acc-0.03' to stop when gradient becomes too small, 0.03 gives buffer\n",
    "            g = self.gradient(X, y, theta, reg_term)\n",
    "            d_theta = (1-beta)*g + beta*d_theta  # momentum\n",
    "            theta = theta-learning_rate*d_theta\n",
    "            cv_pred = self.predict(cv_X, theta)\n",
    "            prev_cv_acc = cv_acc\n",
    "            cv_acc = self.accuracy(cv_pred, cv_y)\n",
    "            if cv_acc > max_cv_acc:  # checks if maximum accuracy thus far\n",
    "                max_cv_acc = cv_acc\n",
    "                best_theta = theta\n",
    "            iterate+=1\n",
    "            if iterate > max_iter:  # since it may not always converge, place a hard ceiling on number of iterations\n",
    "                break\n",
    "        print(max_cv_acc)\n",
    "        print(cv_acc)\n",
    "        return best_theta\n",
    "    \n",
    "    def get_test_acc(self, test_X, test_y, thetas):\n",
    "        test_y = np.reshape(test_y, (-1,1))\n",
    "        \n",
    "        return self.accuracy(self.predict(test_X, thetas), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211, 35)\n",
      "(65, 35)\n",
      "(75, 35)\n",
      "0.8666666666666667\n",
      "0.8533333333333334\n",
      "0.8307692307692308\n"
     ]
    }
   ],
   "source": [
    "new_input = pd.read_csv('ionosphere.data', header=None)\n",
    "new_input[len(new_input.T)-1] = new_input[len(new_input.T)-1].map({'g': 1, 'b':0})\n",
    "new_input.insert(0, column='Bias', value=1)\n",
    "log_reg = LogisticRegression(new_input, percentage=0.6)\n",
    "print(log_reg.testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass: 2799 missing\n",
      "occupation: 2809 missing\n",
      "native-country: 857 missing\n"
     ]
    }
   ],
   "source": [
    "header = ['age','workclass','fnlwgt','education','education_num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "train_df = pd.read_csv('adult.data', names = header)\n",
    "train_df.insert(0, column='Bias', value=1)\n",
    "test_df = pd.read_csv('adult.test', comment = '|', names = header)\n",
    "test_df.insert(0, column='Bias', value=1)\n",
    "adult = pd.concat([test_df, train_df])\n",
    "\n",
    "adult.isnull().sum() #no NaN \n",
    "\n",
    "for i,j in zip(adult.columns,(adult.values.astype(str) == ' ?').sum(axis = 0)):\n",
    "    if j > 0:\n",
    "        print(str(i) + ': ' + str(j) + ' missing')\n",
    "        \n",
    "all_data = [train_df, test_df]\n",
    "for data in all_data:\n",
    "    for i in data.columns:\n",
    "        data[i].replace(' ?', np.nan, inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "adult['income']=adult['income'].map({' <=50K': 0, ' >50K': 1, ' <=50K.': 0, ' >50K.': 1})\n",
    "\n",
    "train_df['income'] = train_df['income'].map({' <=50K': 0, ' >50K': 1})\n",
    "test_df['income'] = test_df['income'].map({' <=50K.': 0, ' >50K.': 1}) \n",
    "\n",
    "columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), [2,4,6,7,8,9,10,14])], remainder='passthrough')\n",
    "onehotencoder = OneHotEncoder()\n",
    "X_train_adult = columnTransformer.fit_transform(train_df).toarray()\n",
    "X_test_adult = columnTransformer.transform(test_df).toarray()\n",
    "\n",
    "\n",
    "y_train_adult = train_df.iloc[:,-1]\n",
    "y_test_adult = test_df.iloc[:,-1]\n",
    "# Encode '<=50k' as 0, '>50k' as 1\n",
    "\n",
    "# y_train_adult = y_train_adult.map({' <=50K': 0, ' >50K': 1})\n",
    "# y_test_adult = y_test_adult.map({' <=50K.': 0, ' >50K.': 1})\n",
    "# y_train_adult=y_train_adult.array\n",
    "# y_test_adult=y_test_adult.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_adult = y_test_adult[:-1]\n",
    "# y_train_adult = y_train_adult[:-1]\n",
    "new_X_train_adult = pd.DataFrame(data = X_train_adult, index = [index for index, row in y_train_adult.iteritems()], columns = [i for i in range(X_train_adult.shape[1])])\n",
    "new_X_test_adult = pd.DataFrame(data = X_test_adult, index = [index for index, row in y_test_adult.iteritems()], columns = [i for i in range(X_test_adult.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_log_reg = LogisticRegression(new_input, train_X = new_X_train_adult, train_y = y_train_adult, test_X = new_X_test_adult, test_y = y_test_adult, percentage=0.6, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7837126693856071\n"
     ]
    }
   ],
   "source": [
    "print(adult_log_reg.testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
