{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, data, percentage=0.5):  # percentage = train/cv+test split\n",
    "        self.data = data\n",
    "        self.percentage = percentage\n",
    "        \n",
    "        self.train_X, self.train_y, self.sub_X, self.sub_y = self.split_data(self.data, self.percentage)\n",
    "        \n",
    "        self.test_X, self.test_y, self.cv_X, self.cv_y = self.split_data(pd.concat([self.sub_X,self.sub_y], axis=1), 0.5)\n",
    "                \n",
    "        self.thetas = self.gradient_descent(self.train_X.values, self.train_y.values, self.cv_X.values, self.cv_y.values)\n",
    "        \n",
    "        self.testing_accuracy = self.get_test_acc(self.test_X.values, self.test_y.values, self.thetas)\n",
    "        \n",
    "    def split_data(self, data, percentage=0.5):\n",
    "        val = np.random.rand(len(data)) < percentage  #splits data and sorts into x, y values\n",
    "        train = data[val]\n",
    "        test = data[~val]\n",
    "\n",
    "        train_X = train.iloc[:, :-1]\n",
    "        train_y = train.iloc[:, -1]\n",
    "\n",
    "        test_X = test.iloc[:, :-1]\n",
    "        test_y = test.iloc[:, -1]\n",
    "        return train_X, train_y ,test_X, test_y\n",
    "    \n",
    "    def predict_proba(self, X, theta):\n",
    "        return self.sigmoid(np.dot(X, theta))\n",
    "\n",
    "    def predict(self, X, theta):\n",
    "        prediction = self.predict_proba(X, theta)\n",
    "        predict_arr = []\n",
    "        for i in prediction:\n",
    "            if i>=0.5:\n",
    "                predict_arr.append(1)\n",
    "            else:\n",
    "                predict_arr.append(0)\n",
    "\n",
    "        return predict_arr\n",
    "\n",
    "    def accuracy(self, predict_arr, y):\n",
    "        correct = 0\n",
    "        for i,j in zip(predict_arr, y):\n",
    "            if i==j[0]:\n",
    "                correct+=1\n",
    "        return correct/len(y)  # accuracy = # tp+tn / total\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def gradient(self, X, y, theta, lambdaa):  # lambdaa is regularization term\n",
    "        N, D = len(X[0]), len(X[0])\n",
    "        yh = self.sigmoid(np.dot(X, theta))\n",
    "        grad = np.dot(X.T, yh-y) / N\n",
    "        grad[1:] += lambdaa * theta[1:]\n",
    "        return grad\n",
    "\n",
    "    def gradient_descent(self, X, y, cv_X, cv_y, learning_rate=0.01, max_iter=50000, beta=0.99, reg_term=0.5):  # attempted termination condition - lack of improvement in cross validation set\n",
    "        N, D = len(X[0]), len(X[0])\n",
    "        theta = np.zeros((len(X[0]), 1))\n",
    "        y = np.reshape(y, (-1,1))  # creates two-dimensional array\n",
    "        cv_y = np.reshape(cv_y, (-1,1))\n",
    "        iterate, cv_acc, prev_cv_acc, d_theta = 0, 0, 0, 0\n",
    "        max_cv_acc = 0  # maximum cross validation accuracy - records thetas at highest cv_acc \n",
    "        best_theta = theta\n",
    "        g = np.inf\n",
    "        eps = 1e-2\n",
    "        while (np.linalg.norm(g) > eps):  # can add in 'or cv_acc>=prev_cv_acc-0.03' to stop when gradient becomes too small, 0.03 gives buffer\n",
    "            g = self.gradient(X, y, theta, reg_term)\n",
    "            d_theta = (1-beta)*g + beta*d_theta  # momentum\n",
    "            theta = theta-learning_rate*d_theta\n",
    "            cv_pred = self.predict(cv_X, theta)\n",
    "            prev_cv_acc = cv_acc\n",
    "            cv_acc = self.accuracy(cv_pred, cv_y)\n",
    "            if cv_acc > max_cv_acc:  # checks if maximum accuracy thus far\n",
    "                max_cv_acc = cv_acc\n",
    "                best_theta = theta\n",
    "            iterate+=1\n",
    "            if iterate > max_iter:  # since it may not always converge, place a hard ceiling on number of iterations\n",
    "                break\n",
    "        print(max_cv_acc)\n",
    "        print(cv_acc)\n",
    "        return best_theta\n",
    "    \n",
    "    def get_test_acc(self, test_X, test_y, thetas):\n",
    "        test_y = np.reshape(test_y, (-1,1))\n",
    "        \n",
    "        return self.accuracy(self.predict(test_X, thetas), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = pd.read_csv('ionosphere.data', header=None)\n",
    "new_input[len(new_input.T)-1] = new_input[len(new_input.T)-1].map({'g': 1, 'b':0})\n",
    "new_input.insert(0, column='Bias', value=1)\n",
    "log_reg = LogisticRegression(new_input)\n",
    "print(log_reg.testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
